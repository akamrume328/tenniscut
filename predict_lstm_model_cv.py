# predict_lstm_model_cv.py (ä¿®æ­£å¾Œ)

import json
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import joblib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Generator
import argparse
from model import TennisLSTMModel  # ãƒ¢ãƒ‡ãƒ«å®šç¾©ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# ===== GPUè¨­å®š =====
def setup_gpu_config():
    """GPUè¨­å®šã¨CUDAæœ€é©åŒ–"""
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        print(f"GPUæ¤œå‡º: {device_count}å°")
        for i in range(device_count):
            gpu_name = torch.cuda.get_device_name(i)
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({memory_total:.1f}GB)")
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.enabled = True
        return torch.device('cuda')
    else:
        print("è­¦å‘Š: GPUæœªæ¤œå‡º: CPUã§å®Ÿè¡Œã—ã¾ã™")
        return torch.device('cpu')

DEVICE = setup_gpu_config()

# ===== æ¨è«–ã‚¯ãƒ©ã‚¹ =====
class TennisLSTMPredictor:
    def __init__(self, models_dir: str = "./models/lstm_model",
                 input_features_dir: str = "./tennis_pipeline_output/02_extracted_features"):
        self.models_dir = Path(models_dir)
        self.input_features_dir = Path(input_features_dir)
        self.predictions_output_dir = Path("./tennis_pipeline_output/03_lstm_predictions")
        self.predictions_output_dir.mkdir(parents=True, exist_ok=True)

        self.model: Optional[TennisLSTMModel] = None
        self.scaler = None
        self.metadata: Optional[Dict] = None
        self.device = DEVICE
        
        self.phase_labels: List[str] = []
        self.feature_names: List[str] = []
        self.sequence_length: int = 30
        self.label_map_inv: Optional[Dict[int, str]] = None

    def select_model_files(self) -> Optional[Tuple[Path, Path, Path]]:
        """å¯¾è©±çš„ã«ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã™ã‚‹"""
        print(f"\n=== å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ ===")
        all_model_files = sorted(
            list(self.models_dir.glob("**/tennis_pytorch*_model.pth")),
            key=lambda p: p.stat().st_mtime,
            reverse=True
        )

        if not all_model_files:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (*.pth) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ in {self.models_dir} ãŠã‚ˆã³ãã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚")
            return None

        valid_sets = []
        for mf_path in all_model_files:
            parent_dir = mf_path.parent
            if not mf_path.name.endswith("_model.pth"):
                continue

            base_name = mf_path.name.removesuffix("_model.pth")
            scaler_path = parent_dir / f"{base_name}_scaler.pkl"
            meta_path = parent_dir / f"{base_name}_metadata.json"
            
            if scaler_path.exists() and meta_path.exists():
                valid_sets.append((mf_path, scaler_path, meta_path))

        if not valid_sets:
            print(f"âŒ å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆï¼ˆscaler, metadataãŒæƒã£ã¦ã„ã‚‹ã‚‚ã®ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None

        for i, (mf_path, _, _) in enumerate(valid_sets, 1):
            try:
                display_path = mf_path.relative_to(self.models_dir)
            except ValueError:
                display_path = mf_path
            print(f"  {i}. {display_path} (æ›´æ–°æ—¥æ™‚: {datetime.fromtimestamp(mf_path.stat().st_mtime):%Y-%m-%d %H:%M})")
        
        try:
            choice = input(f"é¸æŠã—ã¦ãã ã•ã„ (1-{len(valid_sets)}): ").strip()
            choice_num = int(choice)
            return valid_sets[choice_num - 1] if 1 <= choice_num <= len(valid_sets) else None
        except (ValueError, IndexError):
            print("ç„¡åŠ¹ãªå…¥åŠ›ã§ã™ã€‚")
            return None

    def find_model_files_in_set_dir(self, model_set_dir: Path) -> Optional[Tuple[Path, Path, Path]]:
        """æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’æ¤œç´¢ã™ã‚‹"""
        print(f"--- ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¤œç´¢: {model_set_dir} ---")
        model_files = sorted(list(model_set_dir.glob("tennis_pytorch*_model.pth")), key=lambda p: p.stat().st_mtime, reverse=True)
        if not model_files:
            print(f"âŒ {model_set_dir} ã«ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (*_model.pth) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None
        
        latest_model_file = model_files[0]
        base_name = latest_model_file.name.removesuffix("_model.pth")
        
        scaler_path = model_set_dir / f"{base_name}_scaler.pkl"
        meta_path = model_set_dir / f"{base_name}_metadata.json"

        if scaler_path.exists() and meta_path.exists():
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {latest_model_file.name}")
            return latest_model_file, scaler_path, meta_path
        else:
            print(f"âŒ {model_set_dir} ã«å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆ (scaler or metadata) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None

    def load_model_and_metadata(self, model_path: Path, scaler_path: Path, metadata_path: Path) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
        print(f"\n--- ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---")
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f: self.metadata = json.load(f)
            print(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {metadata_path.name}")

            self.scaler = joblib.load(scaler_path)
            print(f"âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼èª­ã¿è¾¼ã¿: {scaler_path.name}")
            
            model_config = self.metadata['model_config']
            
            self.model = TennisLSTMModel(
                input_size=len(self.metadata['feature_names']),
                num_classes=len(self.metadata['phase_labels']),
                hidden_sizes=model_config['lstm_units'],
                dropout_rate=model_config.get('dropout_rate', 0.3),
                model_type=model_config.get('model_type', 'bidirectional'),
                use_batch_norm=model_config.get('batch_size', 64) > 1,
                enable_confidence_weighting=model_config.get('enable_confidence_weighting', False)
            )

            state_dict = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(state_dict)
            self.model.to(self.device)
            self.model.eval()
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_path.name}")

            self.phase_labels = self.metadata['phase_labels']
            self.feature_names = self.metadata['feature_names']
            self.sequence_length = self.metadata.get('sequence_length', model_config.get('sequence_length', 30))
            self.label_map_inv = {i: label_name for i, label_name in enumerate(self.phase_labels)}
            
            return True
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«/ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback; traceback.print_exc()
            return False

    def select_input_feature_file(self) -> Optional[Path]:
        """å¯¾è©±çš„ã«ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã™ã‚‹"""
        print(f"\n=== æ¨è«–ç”¨ ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ ===")
        search_dirs = [self.input_features_dir, Path("./tennis_pipeline_output/02_extracted_features")]
        feature_files = []
        for sdir in search_dirs:
            if sdir.exists():
                feature_files.extend(list(sdir.glob("tennis_inference_features_*.csv")))
        
        feature_files = sorted(list(set(feature_files)), key=lambda p: p.stat().st_mtime, reverse=True)

        if not feature_files:
            print(f"âŒ æ¨è«–ç”¨ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ« (*.csv) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return None

        for i, f_path in enumerate(feature_files, 1):
            print(f"  {i}. {f_path.name}")
        
        try:
            choice = input(f"é¸æŠã—ã¦ãã ã•ã„ (1-{len(feature_files)}): ").strip()
            choice_num = int(choice)
            return feature_files[choice_num - 1] if 1 <= choice_num <= len(feature_files) else None
        except (ValueError, IndexError):
            return None

    def _generate_sequences_for_inference(self, X_scaled: np.ndarray, confidence_scores: Optional[np.ndarray]) -> Generator:
        """æ¨è«–ç”¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿"""
        num_frames = X_scaled.shape[0]
        if num_frames < self.sequence_length:
            return
        for i in range(num_frames - self.sequence_length + 1):
            seq_X = X_scaled[i : i + self.sequence_length]
            original_idx = i + self.sequence_length - 1
            seq_conf = confidence_scores[i : i + self.sequence_length] if confidence_scores is not None else None
            yield seq_X, original_idx, seq_conf

    def prepare_input_data(self, csv_path: Path) -> Tuple[Optional[np.ndarray], Optional[pd.DataFrame], Optional[np.ndarray]]:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹"""
        print(f"\n--- å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æº–å‚™: {csv_path.name} ---")
        try:
            df = pd.read_csv(csv_path)
            print(f"âœ… CSVèª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ")

            missing_features = [f for f in self.feature_names if f not in df.columns]
            if missing_features:
                print(f"âŒ å¿…è¦ãªç‰¹å¾´é‡ãŒCSVã«ã‚ã‚Šã¾ã›ã‚“: {missing_features[:5]}...")
                return None, None, None
            
            X_df = df[self.feature_names].copy().fillna(0).replace([np.inf, -np.inf], 0)
            X_scaled = self.scaler.transform(X_df).astype(np.float32)

            confidence_scores = None
            if self.model and self.model.enable_confidence_weighting:
                if 'interpolation_ratio' in df.columns:
                    # è£œé–“ç‡ãŒä½ã„ã»ã©ä¿¡é ¼åº¦ãŒé«˜ã„ (1 - ratio)
                    confidence_scores = (1.0 - df['interpolation_ratio'].fillna(0)).astype(np.float32).values
                else:
                    confidence_scores = np.ones(len(df), dtype=np.float32)
            
            return X_scaled, df, confidence_scores
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None, None

    def predict(self, X_scaled: np.ndarray, confidence_scores: Optional[np.ndarray], batch_size: int = 256) -> Optional[Tuple]:
        """ãƒãƒƒãƒå‡¦ç†ã§äºˆæ¸¬ã‚’å®Ÿè¡Œã™ã‚‹"""
        if not self.model: return None
        print(f"\n--- æ¨è«–å®Ÿè¡Œ (ãƒãƒƒãƒå‡¦ç†) ---")
        self.model.eval()
        
        all_preds, all_probas, all_indices = [], [], []
        seq_generator = self._generate_sequences_for_inference(X_scaled, confidence_scores)
        
        batch_seq, batch_conf, batch_idx = [], [], []
        for seq_X, original_idx, seq_conf in seq_generator:
            batch_seq.append(seq_X)
            batch_idx.append(original_idx)
            if seq_conf is not None: batch_conf.append(seq_conf)

            if len(batch_seq) == batch_size:
                self._process_batch(batch_seq, batch_conf, batch_idx, all_preds, all_probas, all_indices)
                batch_seq, batch_conf, batch_idx = [], [], []

        if batch_seq:
            self._process_batch(batch_seq, batch_conf, batch_idx, all_preds, all_probas, all_indices)

        if not all_preds:
            return np.array([]), np.array([]), []

        print(f"âœ… æ¨è«–å®Œäº†: {len(all_preds)}ä»¶")
        return np.array(all_preds), np.array(all_probas), all_indices

    def _process_batch(self, batch_seq, batch_conf, batch_idx, all_preds, all_probas, all_indices):
        """1ãƒãƒƒãƒåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹"""
        batch_X_tensor = torch.from_numpy(np.array(batch_seq, dtype=np.float32)).to(self.device)
        batch_conf_tensor = torch.from_numpy(np.array(batch_conf, dtype=np.float32)).to(self.device) if batch_conf else None
        
        with torch.no_grad():
            outputs = self.model(batch_X_tensor, batch_conf_tensor)
            probas = F.softmax(outputs, dim=1)
            _, preds = torch.max(probas, 1)
        
        all_preds.extend(preds.cpu().numpy())
        all_probas.extend(probas.cpu().numpy())
        all_indices.extend(batch_idx)

    def format_predictions(self, predictions: np.ndarray, probabilities: np.ndarray, original_df: pd.DataFrame, original_indices: List[int]) -> pd.DataFrame:
        """äºˆæ¸¬çµæœã‚’DataFrameã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹"""
        if not self.label_map_inv: return original_df
        
        results_df = original_df.copy()
        results_df['predicted_phase'] = ""
        results_df['prediction_confidence'] = np.nan

        pred_series = pd.Series([self.label_map_inv.get(p, f"Unknown_{p}") for p in predictions], index=original_indices)
        conf_series = pd.Series(np.max(probabilities, axis=1), index=original_indices)
        
        results_df.loc[original_indices, 'predicted_phase'] = pred_series
        results_df.loc[original_indices, 'prediction_confidence'] = conf_series
        
        return results_df

    def save_predictions(self, predictions_df: pd.DataFrame, input_filename: str) -> Path:
        """äºˆæ¸¬çµæœã‚’CSVã«ä¿å­˜ã™ã‚‹"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = Path(input_filename).stem.replace("tennis_inference_features_", "")
        output_filename = f"{base_name}_predictions_{timestamp}.csv"
        output_path = self.predictions_output_dir / output_filename
        
        predictions_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"âœ… äºˆæ¸¬çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
        return output_path

    # â˜…â˜…â˜… ã“ã“ã‹ã‚‰ãŒè¿½åŠ ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ â˜…â˜…â˜…
    def run_prediction_for_file(self, model_set_path: Path, feature_csv_path: Path) -> Optional[Path]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã¨ç‰¹å¾´é‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦äºˆæ¸¬ã‚’å®Ÿè¡Œã—ã€çµæœã®CSVãƒ‘ã‚¹ã‚’è¿”ã™ã€‚
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‹ã‚‰ã®å‘¼ã³å‡ºã—ç”¨ã€‚
        """
        print(f"\n=== éå¯¾è©±çš„æ¨è«–é–‹å§‹ ===")
        print(f"ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆãƒ‘ã‚¹: {model_set_path}")
        print(f"ç‰¹å¾´é‡CSVãƒ‘ã‚¹: {feature_csv_path}")

        # 1. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹å®šã¨èª­ã¿è¾¼ã¿
        model_files = self.find_model_files_in_set_dir(model_set_path)
        if not model_files:
            print(f"âŒ æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ {model_set_path} ã§ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
        
        if not self.load_model_and_metadata(*model_files):
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {model_files[0].name}")
            return None

        # 2. ãƒ‡ãƒ¼ã‚¿æº–å‚™
        X_scaled, original_df, confidence_scores = self.prepare_input_data(feature_csv_path)
        if X_scaled is None or original_df is None:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ: {feature_csv_path.name}")
            return None

        # 3. æ¨è«–å®Ÿè¡Œ
        prediction_results = self.predict(X_scaled, confidence_scores)
        if prediction_results is None:
            print("âŒ æ¨è«–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return None
        
        raw_preds, raw_probas, all_original_indices = prediction_results
        
        # 4. çµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatted_df = self.format_predictions(
            predictions=raw_preds, 
            probabilities=raw_probas, 
            original_df=original_df, 
            original_indices=all_original_indices
        )
        
        # 5. çµæœä¿å­˜
        output_csv_path = self.save_predictions(formatted_df, feature_csv_path.name)
        
        print(f"\nğŸ‰ éå¯¾è©±çš„æ¨è«–å®Œäº†ï¼çµæœ: {output_csv_path}")
        return output_csv_path
    # â˜…â˜…â˜… ã“ã“ã¾ã§ãŒè¿½åŠ ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ â˜…â˜…â˜…

    def run_prediction_pipeline(self):
        """å¯¾è©±çš„ã«æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        selected_files = self.select_model_files()
        if not selected_files: return
        
        if not self.load_model_and_metadata(*selected_files): return

        input_csv_path = self.select_input_feature_file()
        if not input_csv_path: return

        X_scaled, original_df, confidence_scores = self.prepare_input_data(input_csv_path)
        if X_scaled is None or original_df is None: return

        prediction_results = self.predict(X_scaled, confidence_scores)
        if prediction_results is None: return
        
        raw_preds, raw_probas, all_original_indices = prediction_results
        formatted_df = self.format_predictions(
            predictions=raw_preds, 
            probabilities=raw_probas, 
            original_df=original_df, 
            original_indices=all_original_indices
        )
        self.save_predictions(formatted_df, input_csv_path.name)

        print("\nğŸ‰ æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†ï¼")

if __name__ == "__main__":
    print("=== ãƒ†ãƒ‹ã‚¹å‹•ç”»å±€é¢åˆ†é¡PyTorch LSTM æ¨è«–ãƒ„ãƒ¼ãƒ« ===")
    predictor = TennisLSTMPredictor()
    
    try:
        predictor.run_prediction_pipeline()
    except KeyboardInterrupt:
        print("\næ“ä½œãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()